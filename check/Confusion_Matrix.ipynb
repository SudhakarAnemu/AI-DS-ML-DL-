{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Confusion_Matrix.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1r2D-47yqbC",
        "colab_type": "text"
      },
      "source": [
        "# \t\t(((Confusion Matrix)))\n",
        "\n",
        "A It will give more details about your analysis. Confusion matrix is a table that is often used to describe the performace of a claffication model(or \"classified\") on a set of test data for which the true valuses are known. \n",
        "1. Is only for classification model there are not used for regression model(model predicts numbers).\n",
        "2. Can only be computed if you have to know the predicted AND true values in order to use a confusion matrix, \n",
        "3. This is a binary classifier (2 classes), which is why it's 2x2. \"Actual\" values are also called \"reference\" or \"true\" values. \n",
        "4. This is the scikit-learn layout for the confusion matrix: classes are listed in alpha order from the top left.\n",
        "\n",
        "\n",
        "n| Predicted Died | Predicted Survived\n",
        "-|----------------|---------------\n",
        "Actual Died|95|11\n",
        "Actual Survived|25|48\n",
        "\n",
        "\n",
        "\n",
        "n = 179|Predicted Died – NO(Negative class)|Predicted Survived – YES(postive class)|sum\n",
        "-|----------------|---------------|---\n",
        "Actual Died – NO<br>(Negative class)|95<br>(TN)|11<br>(FP)<br>Type 1 error's|106\n",
        "Actual Survived – YES<br>(Positive class)|25<br>(FN)<br>(Falsly predicted the Negative Class).<br>Type 2 error's|48(TP)|73\n",
        "sum|120|59\n",
        "\n",
        "Note - FP = Falsely predicted positive(just to remember).\n",
        "\n",
        "<br>a. total number of predections are 179(by adding all four numbers).\n",
        "<br>b. Predicted yes = 59, predicted no = 120, actual no = 106 and actual yes = 73\n",
        "<br>c. if we have more than two classes then do not use the terminology TN, TP etc\n",
        "\n",
        "Model evaluation Metrix : \n",
        "\n",
        "1. Accuracy : Overall how often is the classifier is correct ?\n",
        "(TN + TP)/n = (95 + 48)/179 = 143/179 = 0.79888 = 80 %\n",
        "2. Misclassification Rate : overall, how often is it wrong?\n",
        "(FP+FN)/n = (11+25)/179 = 36/179 = 0.20111 = 20% (100 – above value)\n",
        "3. True Positive Rate(recall, sensitiviey)  : when it is actually yes, how ofthen does it predict yes?\n",
        "TP/actual yes = 48 / 73 = 0.6575 = 66%\n",
        "4. False positive Rate – when it is actually no, how often does it predict yes?\n",
        "FP/actual no = 11/106 = 0.1037 \n",
        "5. True Negative Rate(Specificity) – when it is actually no, how often does ti predict no?\n",
        "TN/n = 95/106=0.8962 (1 – above number)\n",
        "6. Precison : when it predict yes, how often is it correct?\n",
        "TP/predicted yes = 48/59 = 0.8135\n",
        "7. Prevalence : how often does the yes condition actually occur in our sample?\n",
        "Actual yes / total = 59 / 179 = 0.3296\n",
        "\n",
        "Suggested points : \n",
        "I/We need to update this document with respect to the Business perspective\n"
      ]
    }
  ]
}
